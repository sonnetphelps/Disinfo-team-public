{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News API Headline Monitor\n",
    "### Designed for the Disinformation Team at UC Berkeley Human Rights Investigations Lab\n",
    "\n",
    "This notebook uses [News API](newsapi.org) and [newspaper](https://newspaper.readthedocs.io/en/latest/) to monitor current top English-language headlines in a particular country from among News API's 50 options. It downloads the headlines, full text, and NLP-generated keywords of the current 40 most popular articles from among the news sources News API monitors in the given country into a Google Sheet using [pygsheets](https://pygsheets.readthedocs.io/en/stable/spreadsheet.html). It also calculates polarity scores reflecting positive or negative sentiment in the articles' headlines and generates a list of the top `n` most frequently appearing words among both the headlines and article keywords.  \n",
    "\n",
    "This information can help provide a snapshot of the current events in a given country and inform further searching to examine the social media conversations surrounding current events. Because of the current limitation to English-language content, it cannot considered an exhaustive representation of the news, but rather a limited, preliminary summary intended to support additional investigation. \n",
    "\n",
    "_Created by Sonnet Phelps. [Contact](sonnet@berkeley.edu) for more details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "You should have pygsheets credentials saved in a JSON file entitled `pygsheets_authfile.json` and located in the same folder as this notebook in order to access Google Sheets. Read [this walkthrough](https://pygsheets.readthedocs.io/en/stable/authorization.html) for help setting up your API credentials.  \n",
    "\n",
    "You will also need to set up a NewsAPI account and save your API key in a JSON file entitled `newsapi_key.json`. You can register for your API key [here](https://newsapi.org/register)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Setup Instructions:\n",
    "1. Run the entire notebook `(Cell > Run All)` to set it up.\n",
    "2. Uncomment the bottom line in the cell below (delete the `#` in front of the last line).\n",
    "3. Run the cell below. It will take a while.\n",
    "4. Go to __[this Google Sheet](http://https://docs.google.com/spreadsheets/d/1z1ZQnSZrWAKbqbNlr9uIJGbVoeaSJrTDh0dPdaR6Zuc/edit#gid=1361559015)__ to view your newly downloaded articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20 most common words among the headlines are: ['india', 'firstpost', 'attack', 'ndtv', 'pm', 'modi', 'world', 'could', 'news18', '2019', 'new', 'used', 'bjp', 'singh', 'cup', 'hindu', 'hit', '8', 'gwadar', 'men']\n",
      "The 20 most common words among the keywords are: ['attack', 'election', 'west', 'singh', 'india', 'modi', 'firstpost', 'china', 'used', 'tiny', 'delhi', 'remark', 'state', 'mumbai', 'kings', 'super', 'final', 'ipl', 'runs', 'season']\n",
      "         Word  Frequency\n",
      "0       india          9\n",
      "1   firstpost          7\n",
      "2      attack          5\n",
      "3        ndtv          5\n",
      "4          pm          4\n",
      "5        modi          4\n",
      "6       world          3\n",
      "7       could          3\n",
      "8      news18          3\n",
      "9        2019          3\n",
      "10        new          2\n",
      "11       used          2\n",
      "12        bjp          2\n",
      "13      singh          2\n",
      "14        cup          2\n",
      "15      hindu          2\n",
      "16        hit          2\n",
      "17          8          2\n",
      "18     gwadar          2\n",
      "19        men          2\n",
      "         Word  Frequency\n",
      "0      attack          4\n",
      "1    election          4\n",
      "2        west          3\n",
      "3       singh          3\n",
      "4       india          3\n",
      "5        modi          3\n",
      "6   firstpost          3\n",
      "7       china          3\n",
      "8        used          2\n",
      "9        tiny          2\n",
      "10      delhi          2\n",
      "11     remark          2\n",
      "12      state          2\n",
      "13     mumbai          2\n",
      "14      kings          2\n",
      "15      super          2\n",
      "16      final          2\n",
      "17        ipl          2\n",
      "18       runs          2\n",
      "19     season          2\n"
     ]
    }
   ],
   "source": [
    "def search_and_download(title, n=20):\n",
    "    top_headlines = headline_search()\n",
    "    top_headlines['headline_polarity_score'] = score_strings(list(top_headlines['title']))\n",
    "    top_headlines = download_articles(top_headlines)\n",
    "    df_to_gsheet(top_headlines,title, gc)\n",
    "    generate_frequencies(n, top_headlines);\n",
    "    return top_headlines\n",
    "\n",
    "#you can replace the string in the function call below with the title of your own Google Sheet\n",
    "search_and_download('India News Scraping'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to set up the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and setup\n",
    "import sys\n",
    "import datetime as dt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except:\n",
    "    !{sys.executable} -m pip install requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    import newspaper\n",
    "except:\n",
    "    !{sys.executable} -m pip install newspaper3k\n",
    "    import newspaper\n",
    "\n",
    "try:\n",
    "    import pygsheets\n",
    "except:\n",
    "    !{sys.executable} -m pip install pygsheets\n",
    "    import pygsheets\n",
    "    \n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "except:\n",
    "    !{sys.executable} -m pip install vaderSentiment\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    !{sys.executable} -m pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.update(['says', 'today', 'times', 'news'])\n",
    "\n",
    "gc = pygsheets.authorize(service_file='pygsheets_authfile.json')\n",
    "newsapi_key = json.load(open('newsapi_key.json'))['key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-step walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to download the current top articles in India.\n",
    "\n",
    "#### To modify your article search:\n",
    "Enter `keyword='YOUR SEARCH TERMS'` in the `headline_search()` function call below to specify search terms, or leave it blank to get just the top 100 headlines in India. \n",
    "\n",
    "Enter `keyword='CATEGORY'` in the `headline_search()` function call below to specify a category. The options are: `business` `entertainment` `general` `health` `science` `sports` `technology`.\n",
    "\n",
    "Source: NewsAPI.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_headlines = headline_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to calculate polarity scores for the headlines using the `Vader Sentiment Lexicon` and see a distribution plot of the sentiments.\n",
    "\n",
    "Source: John DeNero and DS100 staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_headlines['headline_polarity_score'] = score_strings(list(top_headlines['title']))\n",
    "#sns.distplot(top_headlines['headline_polarity_score']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to download the text and keywords of the articles using `newspaper`. Make sure you have consistent internet connectivity so that it can pull the text from each url. This one may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_headlines = download_articles(top_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to save your articles to a Google Sheet. To share the sheet with yourself, replace `YOUR EMAIL HERE` with your email address (keep the surrounding `''` quotes) and uncomment the bottom line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sh = df_to_gsheet(all_headlines,'India News Scraping', gc)\n",
    "#sh.share('YOUR EMAIL HERE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to get the `n` most popular words among the headlines and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_frequencies(20, top_headlines);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headline_search(key=newsapi_key, keyword='', date=str(dt.date.today()), \n",
    "                    country='in', category='general'):\n",
    "    '''\n",
    "    Uses NewsAPI.org to pull up to 100 top headlines in a country (default = India).\n",
    "    Returns a dataframe of the articles generated in the request\n",
    "    '''\n",
    "\n",
    "    url = ('https://newsapi.org/v2/top-headlines?'\n",
    "           f'q={keyword}&'\n",
    "           f'country={country}&'\n",
    "           f'category={category}&'\n",
    "           f'from={date}&'\n",
    "           f'sortBy=popularity&'\n",
    "           'pageSize=100&'\n",
    "           f'apiKey={key}')\n",
    "\n",
    "    search_json = requests.get(url).json()\n",
    "    return pd.DataFrame.from_dict(search_json['articles'], orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_gsheet(df, sheet_title, gc):\n",
    "    '''Writes dataframe into a new worksheet in a given Google Sheet'''\n",
    "    sh = gc.open(sheet_title)\n",
    "    wks = sh.add_worksheet(f'Download on {dt.datetime.now().strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "    wks.set_dataframe(df, (1,1))\n",
    "    return sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_strings(strings):\n",
    "    '''Takes in a list of strings and returns a list of their polarity scores from -1 to 1'''\n",
    "    scores = []\n",
    "    for s in strings:\n",
    "        score = analyzer.polarity_scores(s)\n",
    "        scores.append(score['compound'])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_frequencies(n, top_headlines):\n",
    "    wordfreq_headlines = count_frequencies(clean_headlines(top_headlines['title']))\n",
    "    wordfreq_keywords = count_frequencies(clean_keywords(top_headlines['keywords']))\n",
    "\n",
    "    print(f\"The {n} most common words among the headlines are: \" + str(list(wordfreq_headlines['Word'].iloc[0:n])))\n",
    "    print(f\"The {n} most common words among the keywords are: \" + str(list(wordfreq_keywords['Word'].iloc[0:n])))\n",
    "    \n",
    "    pp.pprint(wordfreq_headlines.head(n))\n",
    "    pp.pprint(wordfreq_keywords.head(n))\n",
    "    \n",
    "    return wordfreq_headlines, wordfreq_keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_articles(df):\n",
    "    '''\n",
    "    Takes in a dataframe of articles of which one column is the article urls\n",
    "    Uses newspaper to download and parse the text of all articles and generate a list of keywords\n",
    "    Returns the same dataframe with full_text and keywords columns added\n",
    "    '''\n",
    "\n",
    "    full_text, keywords = [], [] \n",
    "    for url in df['url']:\n",
    "        article = newspaper.Article(url)\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            full_text.append(article.text)\n",
    "            keywords.append(article.keywords)\n",
    "        except:\n",
    "            full_text.append('Failed to download')\n",
    "            keywords.append('Failed to download')\n",
    "            continue\n",
    "\n",
    "    df['keywords'] = keywords    \n",
    "    df['full_text'] = full_text\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequencies(words):\n",
    "    '''\n",
    "    Takes in a list of words\n",
    "    Returns a dataframe with their individual frequencies\n",
    "    '''   \n",
    "    return pd.DataFrame(collections.Counter(words).most_common(), columns=['Word', 'Frequency'])\n",
    "\n",
    "def clean_headlines(headlines_series):\n",
    "    '''\n",
    "    Takes in a pandas Series of headlines\n",
    "    Returns a list of words in the series, excepting stopwords\n",
    "    '''\n",
    "    punct_re = r'[^\\w | \\s]'\n",
    "    headline_list = list(headlines_series.str.lower().str.replace(punct_re, ' '))\n",
    "    headline_string = ' '.join(headline_list)\n",
    "    return filter_stopwords(headline_string, stopwords)\n",
    "  \n",
    "def clean_keywords(keywords_series):\n",
    "    words = []\n",
    "    for item in keywords_series:\n",
    "        if item != 'Failed to download':\n",
    "            words += item\n",
    "    return words\n",
    "    \n",
    "def filter_stopwords(words, stopwords):\n",
    "    '''\n",
    "    Filters stopwords out of a string\n",
    "    (Implemented in clean_headlines)\n",
    "    '''\n",
    "    filtered_words = []\n",
    "    for w in words.split():\n",
    "        if w not in stopwords:\n",
    "            filtered_words.append(w)\n",
    "    return filtered_words\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
